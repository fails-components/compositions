# Setting up fails using Docker compose
## Introduction
You can use the files in this directory to easy setup an installation of fails on a single computer using the *docker-compose* command. Or you can use this configuration as the basis for a more complex setup using other container orchestration tools. For using Kubernetes please look into the helm chart directory. The file assumes that you have a working knowledge of Docker and Docker compose

## The architecture of the Fails and the Docker compose setup
Fails is internally divided into several microservices, which we call handlers.
Each is running in its own container.

Fails needs two database services:
- Mongo DB, which stores users, lectures and lectures boards and does the bookkeeping of assets (including pictures, background pdf)
- Redis DB, which holds lecture boards and most configurations during a lecture and also manages private and public keys for authentification across all handlers. It also transports socket.io messages between different handlers
Both handlers are included as separate containers. Only Mongo DB needs are backup, which is handled by a separate backup container (see documentation of the Backup container for details).

Additionally, assets need to be stored. Assets are user-uploaded data such as pictures or pdfs, which serve as background.
Assets can be stored on a directory included in the Docker compose via a bind mount and served by the nginx container.
Alternatively, assets can be stored in an object storage service, that nowadays almost every cloud provider offers. (Currently, Openswift and S3 storage are supported. Fails was tested on Ovh (Openswift and S3) and OpenTelekomCloud(S3)).
Please find in the migrate assets directory a node package/command, that migrates your data from file storage to object storage and vice versa. Can also be used for backup from external object storage.

HTTPs traffic is routed by a HA proxy, which is built as a separate container from the `loadbalancer` directory, which contains HA Proxy's config file.

The handler's microservices in detail:
### ltihandler
Serves routes under `/lti/` and handles authentication against an LTI provider.
Currently, it is tested with moodle LMS, for which a plugin is available, which adds access to a proprietary  REST API on the LTI handler to inform fails of users, course and activity deletion. In this way, the LMS can completely control fails.
The ltihandler creates and JWT authentication token after the LTI authentification flow, which is later valid for the apphandler to access the fails inside the LMS.
You can configure the ltihandler to read-only mode, which effectively limits permissions to learner level and only allows pdf download.
For configuring the LTI handler in your LMS you have to use the following routes:
```
  Tool URL:
  https://thedomain.com/lti/launch

  Authentication URL:
  https://thedomain.com/lti/login

  Redirect URL:
  https://thedomain.com/lti/launch
```

In principle, it is not very difficult to write a replacement for the LTI handler, if a different frontend either standalone or for website systems such as typo3 is desired.
  
### authhandler
It is possible to directly launch the fails main app (e.g. in a room with a Keyboard on a touch screen, where no private login to the lms is possible) and to authenticate with a login code or QR reader from the LTI activity on another device. 
The authhandler is connected via socket.io and a redis adapter to the apphandler authorizing the request.

### apphandler
Handles the REST api under route /app/ for the application inside the LTI activity in your LMS.
It is connected to assets, mongo db and redis.
It serves the raw data to the clients, e.g. for PDF generations. Note PDFs are fully generated on the client side and cause little load on the server.
Furthermore, polls, pictures and other lecture and course configuration are handled by this component.

Authentification is done by the JWT Token generated by the ltihandler. It can also renew its token.
The apphandler also generates JWT Tokens for launching notepads and screens, handled by the notepadhandler.

### notepadhandler
Handles the running lectures for instructors (notepads as well as screens, notepads can write, screens just show content). 
It is connected via socket.io with the lectureapp.
Mainly the drawing commands are passed (to other notepads and screens, again via socket.io and redis adapter, as well as to student notes) and stored in redis databases. Boards are loaded during the launch from mongo database. However, the majority of settings reside inside the redis database during the lecture.
It handles the complete workflow during a lecture. 
It is authenticated using JWT tokens either generated from apphandler or notepadhandler.
JWT token can be renewed.

### noteshandler
It is the same as notepadhandler but for students.
So it is read-only for lecture notes and only managed additional student activities such as polling, chat questions etc.
Separating instructors and students allows controlling the container resources for instructors and students independently, which prevents disruption of services for instructors due to many students using the software.

### housekeeping
Transfer lectures from redis to MongoDB once a minute. It also deletes lectures from redis.
It is also responsible for deleting lectures and assets from MongoDB or assets if the lecture owner and the LMS activity both are deleted.
Once the container should be sufficient in any case.

### avsdispatcher
The avs dispatcher collects information from your avs routers, to connect the browser clients with them.
You need to assign clients and routers to different regions. 

### staticserver
An nginx-based container serving files under /static/.
This includes the main apps 
- /static/app/ inside the LTI and 
- /static/lecture/ during the lecture

as well as open-source licenses
- /static/oss/
and optionally serves assets using secured links with limited temporal validity.

Furthermore, it serves Jupyter Lite under
- /jupyter/
including a proxy for loading packages and hiding the student's ip:
- /jupyter/proxy/


## Specific setup instructions
The first step is to create a *.env* files with the configuration variables:
```
FAILS_TAG="master" # optional tag of fails container, either a version tag or a branch tag,
#  decides with containers to use, you can also use a version tag such as v1 or v1.1 or v1.1.1

FAILS_KEYS_SECRET="YOURKEYFORJWTKEYGENERATION"
# Static secret only required, if the assets are served via nginx
FAILS_STATIC_SECRET="ASECRETFORUSERUPLOADEDASSETSWITHSECUREDURLS"
# Choose the type of storage for your assets "nginx" (default) or "openstackswift"
FAILS_STATIC_WEBSERV_TYPE="nginx"
# were to save your static file, "fs" (default) for filesystem, or "openstackswift"
FAILS_STATIC_SAVE_TYPE="fs"

# If you use swift storage set the following variables
#FAILS_SWIFT_ACCOUNT="Accountnameofyourswiftbucket"
#FAILS_SWIFT_CONTAINER="ContainerNameofYourURL"
#FAILS_SWIFT_KEY="KeyUsedForSignedURLsForYourSwiftStorage"
#FAILS_SWIFT_BASEURL="https://somestorageprovider.org"
#FAILS_SWIFT_USERNAME="UserNameForAccessingYourBucket"
#FAILS_SWIFT_PASSWORD="PasswordForYourUserName"
#FAILS_SWIFT_AUTH_BASEURL = "https://auth.somestorageprovider.org"
#FAILS_SWIFT_DOMAIN="DomainForYourStorage"
#FAILS_SWIFT_PROJECT="ProjectForYourStorage"

# if you use s3 storage set the following values
FAILS_S3_AK="AKFORKEYGENERATION"
FAILS_S3_SK="SKFORKEYGENERATION"
FAILS_S3_REGION="youregion"
FAILS_S3_BUCKET="yourbucket"
FAILS_S3_HOST="hostofyours3provider"
FAILS_S3_ALTURL="alternativehostnameforyoururls"


FAILS_LMS_LIST="TOPUNIVERSITY|https://yourschool.edu/lti/certs.php|https:/yourschool.edu/lti/token.php|https://yourschool.edu/lti/auth.php|yourschool.edu/ TOPUNIVERSITY2|https://yourschool2.edu/lti/certs.php|https:/yourschool2.edu/lti/token.php|https://yourschool2.edu/lti/auth.php|yourschool2.edu/"
# you can pipe custom support contact info and messages into the system for the lms app
FAILS_APP_CONFIG_JSON="{\"support\": { \"text\": \"Please contact our support at\", \"url\": \"https://fabolous-support.de\"}, \"maintenance\": {\"message\": \"The system is going for maintenace at\"}}"
# furthermore you can configure the jupyter proxy with (allowed sites are sites that are 
# not blocked for access in jupyter, typically you should include domains of your school)
FAILS_JUPYTER_PROXY_CONFIG="{\"allowedSites\": [\"https://domain.of.your-school.edu\"]}"
# Next line downgrades access to learners, effective readonly mode, e.g. for maintenance or phasing in or out fails
#FAILS_ONLY_LEARNERS="1"
# The next line adds admin privileges to a list of users, which are not already admins in the LMS
FAILS_ADDL_ADMINS="username1 username2"

# the next lines config smtp settings for informing admins of avs router shortage, so far
#FAILS_ADMIN_EMAIL_SERVER="mysuper.smtpserver.com"
# the next variables use nodemailers defaults, you can leave them out
#FAILS_ADMIN_EMAIL_SERVER_PORT=587
# 1 means on, what only required if connection needs secure connection from the start, for most STARTTLS is ok
#FAILS_ADMIN_EMAIL_SECURE="1"
#FAILS_ADMIN_EMAIL_SENDER_ADDRESS="no-reply@your-fails-server.com"
#FAILS_ADMIN_EMAIL_ACCOUNT_NAME="account@yoursmtpserver.com"
#FAILS_ADMIN_EMAIL_ACCOUNT_PASSWORD="superdupersecretpassword"
#FAILS_ADMIN_EMAILS_ROOT_ADDRESSES="lazyroot@admins.com,sleepyroot@admins.com"

REDIS_DATA_DIR="/path/to/your/redis/db"
REDIS_PASS="yourredispassword"
# Specifiy the docker volume for your mongo db
# note you have to setup a database `fails` inside the mongodb directory 
# and set username and password manually
MONGO_DATA_VOLUME="volumeforyourmongodb"
MONGO_BACKUP_DIR="/path/to/your/mongodb/backup"
MONGO_USER="usernameforfailsinmongodb"
MONGO_PASS="passwordforthisuser"
#MONGO_OPTIONS="--wiredTigerCacheSizeGB 0.5"

# the regions for your avs router FIXME Add documentation on the format
#REGIONS="test|SHAREDSECRETWITHROUTERS|172.18.0.0/24||"
REGIONS="test|SHAREDSECRETWITHROUTER|||"

# must also be provided, if not stored on file system, but then the dir can be empty
ASSETS_DATA_DIR="/path/to/your/users/assets"

FAILS_COOKIE_KEY="keytogeneratecookiesforstickysessioninloadbalancing"

CERT_FILE="/path/to/your/certificate/file/cert.pem"

FAILS_LMS_COURSE_WHITELIST="9999 8888"
FAILS_HTTP_PORT=80
FAILS_HTTPS_PORT=443

```
*FAILS_LMS_COURSE_WHITELIST* should only be used, if you want to use a whitelist, this is perfect for a limited beta test. *FAILS_HTTP_PORT* and *FAILS_HTTPS_PORT* should only be used if they differ from the default ports.
You can find more details about the configuration variables in the *docker-compose.yml* file or in the source code of the components.

You can pull and build the container images (also updates the container software):
```
docker compose build
docker compose pull
```

You can fire the fails server up (or restart with updated containers) with:
```
docker compose up -d
```
if you have a working docker installation with docker compose.
